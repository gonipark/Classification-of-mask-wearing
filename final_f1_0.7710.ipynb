{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be434629",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from glob import glob\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "import cv2\n",
    "#from albumentations import *\n",
    "import albumentations as A\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# from tqdm.notebook import tqdm\n",
    "import albumentations.pytorch\n",
    "from IPython.display import Audio\n",
    "\n",
    "def seed_everything(seed):\n",
    "    \"\"\"\n",
    "    동일한 조건으로 학습을 할 때, 동일한 결과를 얻기 위해 seed를 고정시킵니다.\n",
    "    \n",
    "    Args:\n",
    "        seed: seed 정수값\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11a43d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '/opt/ml/input/data/train/images'\n",
    "csv_dir = '/opt/ml/input/data/train/train.csv'\n",
    "log_dir   = '/opt/ml/input/data/train/log'\n",
    "test_dir = '/opt/ml/input/data/eval'\n",
    "test_dir_image='/opt/ml/input/data/eval/images'\n",
    "for_semi_dir_possi='/opt/ml/input/data/eval/toensemble/submission_second_semi_supervised_learning_wo_argumetation_1.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48477073",
   "metadata": {},
   "outputs": [],
   "source": [
    "class maskDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, root_dir,csv_dir, is_All=False,is_semi=False,splited_idx=None, transform=None):\n",
    "        self.images_list = self._load_img_list(root_dir,is_All,is_semi,splited_idx)\n",
    "        self.dir_without_filename=self._get_only_dir_without_filename()\n",
    "        self.len=len(self.images_list)\n",
    "        self.root_dir=root_dir\n",
    "        self.csv_dir=csv_dir\n",
    "        self.transform=transform\n",
    "        self.is_semi=is_semi\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        \n",
    "        \n",
    "        img_path=self.images_list[index]\n",
    "        img = Image.open(img_path)\n",
    "        if self.transform:\n",
    "            img=self.transform(image=np.array(img))[\"image\"]\n",
    "            \n",
    "        if not self.is_semi:\n",
    "            label=self._get_class_idx_from_img_name(img_path)\n",
    "        else:\n",
    "            semi_df = pd.read_csv(for_semi_dir_possi)\n",
    "            condi=semi_df['possi']>6.8\n",
    "            semi_df = semi_df[condi]\n",
    "            \n",
    "            ids=semi_df['ans'].tolist()\n",
    "            label=ids[index]\n",
    "        \n",
    "        return img, label\n",
    "            \n",
    "        \n",
    "    def _get_only_dir_without_filename(self):\n",
    "        dir_without_filename=[]\n",
    "        for filename in glob(root_dir+'/*'): \n",
    "            dir_without_filename.append(filename)\n",
    "        \n",
    "        return dir_without_filename\n",
    "    \n",
    "        \n",
    "    def _load_img_list(self, root_dir,is_All,is_semi,splited_idx):\n",
    "        full_dir=glob(root_dir+'/*')\n",
    "        train=[]\n",
    "        valid=[]\n",
    "        images_list=[]\n",
    "        \n",
    "        dir_without_filename=self._get_only_dir_without_filename()\n",
    "            \n",
    "        if is_All :\n",
    "            for filename in dir_without_filename:\n",
    "                images_list.extend(glob(filename+'/*')) \n",
    "                \n",
    "        elif is_semi:\n",
    "            semi_df = pd.read_csv(for_semi_dir_possi)\n",
    "            condi=semi_df['possi']>6.8\n",
    "            semi_df = semi_df[condi]\n",
    "            ids=semi_df['ImageID']\n",
    "            paths=ids.tolist()\n",
    "            for each in paths:\n",
    "                images_list.append(os.path.join(test_dir_image, each))\n",
    "                \n",
    "        else:\n",
    "            for idx in splited_idx:\n",
    "                images_list.extend(glob(dir_without_filename[idx]+'/*'))   \n",
    "\n",
    "\n",
    "        return images_list\n",
    "\n",
    "    def _read_csv(self,csv_dir):\n",
    "        csv_df = pd.read_csv(csv_dir)\n",
    "        \n",
    "        bins_dividers=[0,29,57,61]\n",
    "        bin_names = ['<30','>=30 and <60','>=60']\n",
    "        csv_df['age_bin'] = pd.cut(x = csv_df['age'],\n",
    "                     bins = bins_dividers,   \n",
    "                     labels = bin_names)\n",
    "\n",
    "        return csv_df\n",
    "    \n",
    "    def _load_img_ID(self, img_path):\n",
    "        return int(img_path.split('/')[7].split('_')[0])\n",
    "    \n",
    "    def _get_class_idx_from_img_name(self,img_name):\n",
    "        name_list=img_name.split('/')[7:]\n",
    "        age=int(name_list[0][-2:])\n",
    " \n",
    "        if 'incorrect' in name_list[1]:\n",
    "            if self.isFemale(name_list[0]):\n",
    "                if self.under30(age):\n",
    "                    return 9\n",
    "                elif self.under60(age):\n",
    "                    return 10\n",
    "                else:\n",
    "                    return 11\n",
    "            else:\n",
    "                if self.under30(age):\n",
    "                    return 6\n",
    "                elif self.under60(age):\n",
    "                    return 7\n",
    "                else:\n",
    "                    return 8\n",
    "                \n",
    "                \n",
    "        elif 'mask' in name_list[1]:\n",
    "            \n",
    "            if self.isFemale(name_list[0]):\n",
    "                if self.under30(age):\n",
    "                    return 3\n",
    "                elif self.under60(age):\n",
    "                    return 4\n",
    "                else:\n",
    "                    return 5\n",
    "            else:\n",
    "                if self.under30(age):\n",
    "                    return 0\n",
    "                elif self.under60(age):\n",
    "                    return 1\n",
    "                else:\n",
    "                    return 2\n",
    "            \n",
    "        elif 'normal' in name_list[1]:\n",
    "            if self.isFemale(name_list[0]):\n",
    "                if self.under30(age):\n",
    "                    return 15\n",
    "                elif self.under60(age):\n",
    "                    return 16\n",
    "                else:\n",
    "                    return 17\n",
    "            else:\n",
    "                if self.under30(age):\n",
    "                    return 12\n",
    "                elif self.under60(age):\n",
    "                    return 13\n",
    "                else:\n",
    "                    return 14\n",
    "        else:\n",
    "            raise ValueError(\"%s is not a valid filename. Please change the name of %s.\" % (img_name, img_path))\n",
    "            \n",
    "    def isFemale(self,name_list):\n",
    "        if 'female' in name_list:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def under30(self,age):\n",
    "        if age<30:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def under60(self,age):\n",
    "        if age<58:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9811414",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSplitedLoader(splits):\n",
    "    train_loaders=[]\n",
    "    valid_loaders=[]\n",
    "    for train_idx,valid_idx in splits:\n",
    "    \n",
    "        train_dataset = maskDataset(root_dir,csv_dir, splited_idx=train_idx, transform=transform_train)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, pin_memory=True, shuffle=True,num_workers=8)\n",
    "\n",
    "        valid_dataset = maskDataset(root_dir,csv_dir, splited_idx=valid_idx,transform=transform_valid)\n",
    "        valid_loader = DataLoader(valid_dataset, batch_size=batch_size, pin_memory=True, shuffle=True,num_workers=8)\n",
    "        \n",
    "        train_loaders.append(train_loader)\n",
    "        valid_loaders.append(valid_loader)\n",
    "        \n",
    "    return train_loaders, valid_loaders\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0318cc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "lr = 1e-4\n",
    "input_size = 224\n",
    "\n",
    "transform_train = A.Compose([\n",
    "       A.CenterCrop(300,200,p=0.5),\n",
    "        A.IAAPerspective(),\n",
    "        A.Resize(224,224),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]),\n",
    "    \n",
    "    A.pytorch.ToTensor(),\n",
    "     \n",
    "    ])\n",
    "\n",
    "transform_valid = A.Compose([\n",
    "    \n",
    "    A.Resize(224,224),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]),\n",
    "     A.pytorch.ToTensor(),\n",
    "     \n",
    "    ])\n",
    "\n",
    "all_dataset=maskDataset(root_dir,csv_dir,is_All=True, transform=transform_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7c7e5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "semi_dataset = maskDataset(root_dir,csv_dir, is_semi=True, transform=transform_valid)\n",
    "semi_loader = DataLoader(semi_dataset, batch_size=batch_size, pin_memory=True, shuffle=True,num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c463aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, img_paths, transform):\n",
    "        self.img_paths = img_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.img_paths[index])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "    \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85235c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, weight=None,\n",
    "                 gamma=2., reduction='mean'):\n",
    "        nn.Module.__init__(self)\n",
    "        self.weight = weight\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, input_tensor, target_tensor):\n",
    "        log_prob = F.log_softmax(input_tensor, dim=-1)\n",
    "        prob = torch.exp(log_prob)\n",
    "        return F.nll_loss(\n",
    "            ((1 - prob) ** self.gamma) * log_prob,\n",
    "            target_tensor,\n",
    "            weight=self.weight,\n",
    "            reduction=self.reduction\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ac0acb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and Optimizer\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ed1eb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73daceba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1617858200.609101\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import time\n",
    "start=time.time()\n",
    "print(start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "310625cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /opt/ml/.cache/torch/hub/pytorch_vision_v0.9.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------now 1epoch/1kfold------------------------\n",
      "Iter [  0/473] | Train Loss 8.9825 | Train f1 0.0000 | Valid Loss 8.5655 | Valid f1 0.0003\n",
      "Iter [200/473] | Train Loss 0.2295 | Train f1 0.6417 | Valid Loss 0.3010 | Valid f1 0.6374\n",
      "Iter [222/223] | Train Loss 0.0790 | Train f1 0.7833 | Valid Loss 0.3010 | Valid f1 0.6374\n",
      "Iter [400/473] | Train Loss 0.1254 | Train f1 0.7796 | Valid Loss 0.2548 | Valid f1 0.6533\n",
      "Iter [472/473] | Train Loss 0.0762 | Train f1 0.5800 | Valid Loss 0.2363 | Valid f1 0.6737\n",
      "------------------------now 2epoch/1kfold------------------------\n",
      "Iter [  0/473] | Train Loss 0.1819 | Train f1 0.6962 | Valid Loss 0.2430 | Valid f1 0.6742\n",
      "Iter [200/473] | Train Loss 0.0753 | Train f1 0.9575 | Valid Loss 0.2143 | Valid f1 0.7143\n",
      "Iter [222/223] | Train Loss 0.0018 | Train f1 1.0000 | Valid Loss 0.2143 | Valid f1 0.7143\n",
      "Iter [400/473] | Train Loss 0.0986 | Train f1 0.7134 | Valid Loss 0.2236 | Valid f1 0.6867\n",
      "Iter [472/473] | Train Loss 0.1017 | Train f1 0.6800 | Valid Loss 0.2288 | Valid f1 0.6753\n",
      "------------------------now 3epoch/1kfold------------------------\n",
      "Iter [  0/473] | Train Loss 0.0833 | Train f1 0.7307 | Valid Loss 0.2211 | Valid f1 0.6990\n",
      "Iter [200/473] | Train Loss 0.0443 | Train f1 0.8735 | Valid Loss 0.2720 | Valid f1 0.6842\n",
      "Iter [222/223] | Train Loss 0.0306 | Train f1 0.8333 | Valid Loss 0.2720 | Valid f1 0.6842\n",
      "Iter [400/473] | Train Loss 0.1301 | Train f1 0.5590 | Valid Loss 0.2165 | Valid f1 0.7146\n",
      "Iter [472/473] | Train Loss 0.2415 | Train f1 0.6000 | Valid Loss 0.2674 | Valid f1 0.7004\n",
      "------------------------now 4epoch/1kfold------------------------\n",
      "Iter [  0/473] | Train Loss 0.1785 | Train f1 0.8444 | Valid Loss 0.2584 | Valid f1 0.6960\n",
      "Iter [200/473] | Train Loss 0.0836 | Train f1 0.9603 | Valid Loss 0.2593 | Valid f1 0.6915\n",
      "Iter [222/223] | Train Loss 0.0140 | Train f1 1.0000 | Valid Loss 0.2593 | Valid f1 0.6915\n",
      "Iter [400/473] | Train Loss 0.0640 | Train f1 0.9603 | Valid Loss 0.2695 | Valid f1 0.7040\n",
      "Iter [472/473] | Train Loss 0.0898 | Train f1 0.8296 | Valid Loss 0.2224 | Valid f1 0.7172\n",
      "------------------------now 5epoch/1kfold------------------------\n",
      "Iter [  0/473] | Train Loss 0.0262 | Train f1 0.9758 | Valid Loss 0.2090 | Valid f1 0.7250\n",
      "Iter [200/473] | Train Loss 0.0472 | Train f1 0.9835 | Valid Loss 0.2833 | Valid f1 0.6994\n",
      "Iter [222/223] | Train Loss 0.1367 | Train f1 0.9074 | Valid Loss 0.2833 | Valid f1 0.6994\n",
      "------------------------------------------test inference is done----------------------------------------------\n",
      "2917.0511474609375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /opt/ml/.cache/torch/hub/pytorch_vision_v0.9.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------now 1epoch/2kfold------------------------\n",
      "Iter [  0/473] | Train Loss 9.3603 | Train f1 0.0000 | Valid Loss 8.5585 | Valid f1 0.0001\n",
      "Iter [200/473] | Train Loss 0.0482 | Train f1 0.7316 | Valid Loss 0.3106 | Valid f1 0.6219\n",
      "Iter [222/223] | Train Loss 0.4222 | Train f1 0.7667 | Valid Loss 0.3106 | Valid f1 0.6219\n",
      "Iter [400/473] | Train Loss 0.2569 | Train f1 0.4568 | Valid Loss 0.2614 | Valid f1 0.6675\n",
      "Iter [472/473] | Train Loss 0.0560 | Train f1 1.0000 | Valid Loss 0.2620 | Valid f1 0.6526\n",
      "------------------------now 2epoch/2kfold------------------------\n",
      "Iter [  0/473] | Train Loss 0.1365 | Train f1 0.5737 | Valid Loss 0.2595 | Valid f1 0.6451\n",
      "Iter [200/473] | Train Loss 0.0656 | Train f1 0.9424 | Valid Loss 0.2319 | Valid f1 0.7085\n",
      "Iter [222/223] | Train Loss 0.1802 | Train f1 0.6687 | Valid Loss 0.2319 | Valid f1 0.7085\n",
      "Iter [400/473] | Train Loss 0.0412 | Train f1 0.8889 | Valid Loss 0.2395 | Valid f1 0.7039\n",
      "Iter [472/473] | Train Loss 0.0476 | Train f1 0.8000 | Valid Loss 0.2466 | Valid f1 0.6920\n",
      "------------------------now 3epoch/2kfold------------------------\n",
      "Iter [  0/473] | Train Loss 0.0608 | Train f1 0.8266 | Valid Loss 0.2414 | Valid f1 0.6934\n",
      "Iter [200/473] | Train Loss 0.0748 | Train f1 0.7302 | Valid Loss 0.2520 | Valid f1 0.7008\n",
      "Iter [222/223] | Train Loss 0.0289 | Train f1 0.7778 | Valid Loss 0.2520 | Valid f1 0.7008\n",
      "Iter [400/473] | Train Loss 0.2006 | Train f1 0.8241 | Valid Loss 0.2730 | Valid f1 0.7055\n",
      "Iter [472/473] | Train Loss 0.0522 | Train f1 0.9567 | Valid Loss 0.2702 | Valid f1 0.6854\n",
      "------------------------now 4epoch/2kfold------------------------\n",
      "Iter [  0/473] | Train Loss 0.0398 | Train f1 0.8762 | Valid Loss 0.2716 | Valid f1 0.7018\n",
      "Iter [200/473] | Train Loss 0.1032 | Train f1 0.8899 | Valid Loss 0.3169 | Valid f1 0.7004\n",
      "Iter [222/223] | Train Loss 0.0086 | Train f1 1.0000 | Valid Loss 0.3169 | Valid f1 0.7004\n",
      "Iter [400/473] | Train Loss 0.0467 | Train f1 0.8387 | Valid Loss 0.3118 | Valid f1 0.6985\n",
      "Iter [472/473] | Train Loss 0.0425 | Train f1 0.7778 | Valid Loss 0.2734 | Valid f1 0.6962\n",
      "------------------------now 5epoch/2kfold------------------------\n",
      "Iter [  0/473] | Train Loss 0.2576 | Train f1 0.8023 | Valid Loss 0.2621 | Valid f1 0.7070\n",
      "Iter [200/473] | Train Loss 0.0134 | Train f1 1.0000 | Valid Loss 0.2914 | Valid f1 0.7138\n",
      "Iter [222/223] | Train Loss 0.1387 | Train f1 0.7424 | Valid Loss 0.2914 | Valid f1 0.7138\n",
      "Iter [400/473] | Train Loss 0.0347 | Train f1 1.0000 | Valid Loss 0.2758 | Valid f1 0.7397\n",
      "Iter [472/473] | Train Loss 0.2395 | Train f1 0.9074 | Valid Loss 0.3340 | Valid f1 0.6715\n",
      "------------------------------------------test inference is done----------------------------------------------\n",
      "2811.748896598816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /opt/ml/.cache/torch/hub/pytorch_vision_v0.9.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------now 1epoch/3kfold------------------------\n",
      "Iter [  0/473] | Train Loss 9.0432 | Train f1 0.0000 | Valid Loss 8.6383 | Valid f1 0.0002\n",
      "Iter [200/473] | Train Loss 0.3349 | Train f1 0.5641 | Valid Loss 0.3192 | Valid f1 0.6143\n",
      "Iter [222/223] | Train Loss 0.0594 | Train f1 0.7576 | Valid Loss 0.3192 | Valid f1 0.6143\n",
      "Iter [400/473] | Train Loss 0.1509 | Train f1 0.7428 | Valid Loss 0.2227 | Valid f1 0.6633\n",
      "Iter [472/473] | Train Loss 0.2511 | Train f1 0.6370 | Valid Loss 0.2744 | Valid f1 0.6286\n",
      "------------------------now 2epoch/3kfold------------------------\n",
      "Iter [  0/473] | Train Loss 0.1844 | Train f1 0.5481 | Valid Loss 0.2687 | Valid f1 0.6297\n",
      "Iter [200/473] | Train Loss 0.1825 | Train f1 0.7598 | Valid Loss 0.2499 | Valid f1 0.6301\n",
      "Iter [222/223] | Train Loss 0.1386 | Train f1 0.7333 | Valid Loss 0.2499 | Valid f1 0.6301\n",
      "Iter [400/473] | Train Loss 0.1296 | Train f1 0.8691 | Valid Loss 0.2270 | Valid f1 0.6974\n",
      "Iter [472/473] | Train Loss 0.0847 | Train f1 0.8000 | Valid Loss 0.2269 | Valid f1 0.6772\n",
      "------------------------now 3epoch/3kfold------------------------\n",
      "Iter [  0/473] | Train Loss 0.0965 | Train f1 0.5087 | Valid Loss 0.2351 | Valid f1 0.6792\n",
      "Iter [200/473] | Train Loss 0.1040 | Train f1 0.8974 | Valid Loss 0.2401 | Valid f1 0.6786\n",
      "Iter [222/223] | Train Loss 0.0638 | Train f1 0.9320 | Valid Loss 0.2401 | Valid f1 0.6786\n",
      "Iter [400/473] | Train Loss 0.0612 | Train f1 0.8019 | Valid Loss 0.2612 | Valid f1 0.6834\n",
      "Iter [472/473] | Train Loss 0.0327 | Train f1 1.0000 | Valid Loss 0.2623 | Valid f1 0.6814\n",
      "------------------------now 4epoch/3kfold------------------------\n",
      "Iter [  0/473] | Train Loss 0.0475 | Train f1 0.8438 | Valid Loss 0.2708 | Valid f1 0.6756\n",
      "Iter [200/473] | Train Loss 0.0125 | Train f1 0.9882 | Valid Loss 0.2318 | Valid f1 0.7052\n",
      "Iter [222/223] | Train Loss 0.0060 | Train f1 1.0000 | Valid Loss 0.2318 | Valid f1 0.7052\n",
      "Iter [400/473] | Train Loss 0.0960 | Train f1 0.9143 | Valid Loss 0.2596 | Valid f1 0.6847\n",
      "Iter [472/473] | Train Loss 0.0978 | Train f1 0.6508 | Valid Loss 0.3047 | Valid f1 0.6985\n",
      "------------------------now 5epoch/3kfold------------------------\n",
      "Iter [  0/473] | Train Loss 0.0069 | Train f1 1.0000 | Valid Loss 0.3081 | Valid f1 0.6910\n",
      "Iter [200/473] | Train Loss 0.0177 | Train f1 0.8889 | Valid Loss 0.3067 | Valid f1 0.6778\n",
      "Iter [222/223] | Train Loss 0.0045 | Train f1 1.0000 | Valid Loss 0.3067 | Valid f1 0.6778\n",
      "Iter [400/473] | Train Loss 0.0455 | Train f1 0.7333 | Valid Loss 0.2711 | Valid f1 0.6802\n",
      "Iter [472/473] | Train Loss 0.2104 | Train f1 0.8462 | Valid Loss 0.3050 | Valid f1 0.6683\n",
      "------------------------------------------test inference is done----------------------------------------------\n",
      "2806.745579481125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /opt/ml/.cache/torch/hub/pytorch_vision_v0.9.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------now 1epoch/4kfold------------------------\n",
      "Iter [  0/473] | Train Loss 9.2502 | Train f1 0.0000 | Valid Loss 8.5600 | Valid f1 0.0002\n",
      "Iter [200/473] | Train Loss 0.2294 | Train f1 0.6067 | Valid Loss 0.3244 | Valid f1 0.6148\n",
      "Iter [222/223] | Train Loss 0.0516 | Train f1 0.9394 | Valid Loss 0.3244 | Valid f1 0.6148\n",
      "Iter [400/473] | Train Loss 0.2354 | Train f1 0.6341 | Valid Loss 0.2422 | Valid f1 0.6706\n",
      "Iter [472/473] | Train Loss 0.4702 | Train f1 0.3735 | Valid Loss 0.2497 | Valid f1 0.6909\n",
      "------------------------now 2epoch/4kfold------------------------\n",
      "Iter [  0/473] | Train Loss 0.2184 | Train f1 0.7772 | Valid Loss 0.2612 | Valid f1 0.7044\n",
      "Iter [200/473] | Train Loss 0.1467 | Train f1 0.8000 | Valid Loss 0.2110 | Valid f1 0.6881\n",
      "Iter [222/223] | Train Loss 0.0596 | Train f1 0.8424 | Valid Loss 0.2110 | Valid f1 0.6881\n",
      "Iter [400/473] | Train Loss 0.0527 | Train f1 0.9694 | Valid Loss 0.2242 | Valid f1 0.6934\n",
      "Iter [472/473] | Train Loss 0.0278 | Train f1 1.0000 | Valid Loss 0.2486 | Valid f1 0.6976\n",
      "------------------------now 3epoch/4kfold------------------------\n",
      "Iter [  0/473] | Train Loss 0.1486 | Train f1 0.9212 | Valid Loss 0.2375 | Valid f1 0.6982\n",
      "Iter [200/473] | Train Loss 0.0979 | Train f1 0.7303 | Valid Loss 0.2530 | Valid f1 0.6939\n",
      "Iter [222/223] | Train Loss 0.0120 | Train f1 1.0000 | Valid Loss 0.2530 | Valid f1 0.6939\n",
      "Iter [400/473] | Train Loss 0.0695 | Train f1 0.8091 | Valid Loss 0.1902 | Valid f1 0.7454\n",
      "Iter [472/473] | Train Loss 0.0265 | Train f1 0.8333 | Valid Loss 0.2393 | Valid f1 0.7158\n",
      "------------------------now 4epoch/4kfold------------------------\n",
      "Iter [  0/473] | Train Loss 0.0523 | Train f1 0.7956 | Valid Loss 0.2343 | Valid f1 0.7084\n",
      "Iter [200/473] | Train Loss 0.0704 | Train f1 0.8436 | Valid Loss 0.2757 | Valid f1 0.6954\n",
      "Iter [222/223] | Train Loss 0.1028 | Train f1 0.8667 | Valid Loss 0.2757 | Valid f1 0.6954\n",
      "Iter [400/473] | Train Loss 0.0843 | Train f1 0.8500 | Valid Loss 0.3209 | Valid f1 0.6771\n",
      "Iter [472/473] | Train Loss 0.0582 | Train f1 0.7778 | Valid Loss 0.2890 | Valid f1 0.6737\n",
      "------------------------now 5epoch/4kfold------------------------\n",
      "Iter [  0/473] | Train Loss 0.0340 | Train f1 0.9487 | Valid Loss 0.2880 | Valid f1 0.6755\n",
      "Iter [200/473] | Train Loss 0.0370 | Train f1 0.9900 | Valid Loss 0.2649 | Valid f1 0.7122\n",
      "Iter [222/223] | Train Loss 0.0029 | Train f1 1.0000 | Valid Loss 0.2649 | Valid f1 0.7122\n",
      "Iter [400/473] | Train Loss 0.0869 | Train f1 0.8523 | Valid Loss 0.2706 | Valid f1 0.6680\n",
      "Iter [472/473] | Train Loss 0.1308 | Train f1 0.6364 | Valid Loss 0.2606 | Valid f1 0.7022\n",
      "Iter [200/473] | Train Loss 0.1513 | Train f1 0.6704 | Valid Loss 0.3378 | Valid f1 0.6073\n",
      "Iter [222/223] | Train Loss 0.1232 | Train f1 0.6381 | Valid Loss 0.3378 | Valid f1 0.6073\n",
      "Iter [400/473] | Train Loss 0.1254 | Train f1 0.7212 | Valid Loss 0.2556 | Valid f1 0.6650\n",
      "Iter [472/473] | Train Loss 0.2157 | Train f1 0.6877 | Valid Loss 0.2536 | Valid f1 0.6745\n",
      "------------------------now 2epoch/5kfold------------------------\n",
      "Iter [  0/473] | Train Loss 0.1452 | Train f1 0.6920 | Valid Loss 0.2475 | Valid f1 0.6828\n",
      "Iter [200/473] | Train Loss 0.0641 | Train f1 0.8171 | Valid Loss 0.2775 | Valid f1 0.6556\n",
      "Iter [222/223] | Train Loss 0.0147 | Train f1 1.0000 | Valid Loss 0.2775 | Valid f1 0.6556\n",
      "Iter [400/473] | Train Loss 0.0439 | Train f1 0.9196 | Valid Loss 0.2646 | Valid f1 0.6811\n",
      "Iter [472/473] | Train Loss 0.0691 | Train f1 0.9429 | Valid Loss 0.2643 | Valid f1 0.6747\n",
      "------------------------now 3epoch/5kfold------------------------\n",
      "Iter [  0/473] | Train Loss 0.0512 | Train f1 0.8182 | Valid Loss 0.2586 | Valid f1 0.6893\n",
      "Iter [200/473] | Train Loss 0.0362 | Train f1 0.8249 | Valid Loss 0.2972 | Valid f1 0.6972\n",
      "Iter [222/223] | Train Loss 0.0967 | Train f1 0.9467 | Valid Loss 0.2972 | Valid f1 0.6972\n",
      "Iter [400/473] | Train Loss 0.0331 | Train f1 0.8146 | Valid Loss 0.2856 | Valid f1 0.6957\n",
      "Iter [472/473] | Train Loss 0.0399 | Train f1 0.7778 | Valid Loss 0.2734 | Valid f1 0.6689\n",
      "------------------------now 4epoch/5kfold------------------------\n",
      "Iter [  0/473] | Train Loss 0.0622 | Train f1 0.8611 | Valid Loss 0.2736 | Valid f1 0.6712\n",
      "Iter [200/473] | Train Loss 0.0458 | Train f1 0.9143 | Valid Loss 0.3286 | Valid f1 0.6889\n",
      "Iter [222/223] | Train Loss 0.0091 | Train f1 1.0000 | Valid Loss 0.3286 | Valid f1 0.6889\n",
      "Iter [400/473] | Train Loss 0.0126 | Train f1 1.0000 | Valid Loss 0.2880 | Valid f1 0.6711\n",
      "Iter [472/473] | Train Loss 0.0121 | Train f1 1.0000 | Valid Loss 0.3199 | Valid f1 0.6835\n",
      "------------------------now 5epoch/5kfold------------------------\n",
      "Iter [  0/473] | Train Loss 0.0441 | Train f1 0.9505 | Valid Loss 0.3178 | Valid f1 0.6836\n",
      "Iter [200/473] | Train Loss 0.1303 | Train f1 0.7257 | Valid Loss 0.3329 | Valid f1 0.6950\n",
      "Iter [222/223] | Train Loss 0.0182 | Train f1 1.0000 | Valid Loss 0.3329 | Valid f1 0.6950\n",
      "Iter [400/473] | Train Loss 0.0612 | Train f1 0.9303 | Valid Loss 0.3331 | Valid f1 0.6930\n",
      "Iter [472/473] | Train Loss 0.0279 | Train f1 1.0000 | Valid Loss 0.3179 | Valid f1 0.6774\n",
      "------------------------------------------test inference is done----------------------------------------------\n",
      "2938.528119325638\n"
     ]
    }
   ],
   "source": [
    "# Main\n",
    "epochs=5\n",
    "folds=KFold(n_splits=5,shuffle=False)\n",
    "splits=[]\n",
    "for current_fold,(train_idx, valid_idx) in enumerate(folds.split(all_dataset.dir_without_filename)):\n",
    "    splits.append((train_idx,valid_idx))\n",
    "\n",
    "train_loaders, valid_loaders= getSplitedLoader(splits)\n",
    "\n",
    "kfold_num=0\n",
    "for train_loader, valid_loader in zip(train_loaders, valid_loaders):\n",
    "    start=time.time()\n",
    "    kfold_num+=1\n",
    "    #effiNet모델은 여기에서 선언 변하는 것 외에 차이가 없음\n",
    "    model = torch.hub.load('pytorch/vision:v0.9.0', 'resnet50', pretrained=True)\n",
    "    device = torch.device('cuda')\n",
    "    model.to(device)\n",
    "\n",
    "    criterion = FocalLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=2, eta_min=0.)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f\"------------------------now {epoch+1}epoch/{kfold_num}kfold------------------------\")\n",
    "        model.train()\n",
    "        for iter, (img, label) in enumerate(train_loader):\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            img, label = img.float().to(device), label.long().to(device)\n",
    "\n",
    "            pred_logit = model(img)\n",
    "\n",
    "            loss = criterion(pred_logit, label)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            pred_label = torch.argmax(pred_logit, 1)\n",
    "            f1=f1_score(label.to('cpu'), pred_label.to('cpu'),average='macro')\n",
    "\n",
    "            train_loss = loss.item()\n",
    "            train_f1=f1\n",
    "            \n",
    "            #(나름)semi-supervised learning 하는 파트\n",
    "            #어떻게 하는지 잘 모르겠어서 일단 대충 203번쨰 iter에서 원래 보던 데이터들 말고 eval 데이터를 가져와서 봄\n",
    "            #valid했더니 성능은 더 떨어져서 그 부분은 없앰 즉 학습만 하고 다시 기존 train dataset으로 넘어감\n",
    "            if(iter==203):\n",
    "                for plusiter, (img, label) in enumerate(semi_loader):\n",
    "                    optimizer.zero_grad()\n",
    "                    img, label = img.float().to(device), label.long().to(device)\n",
    "\n",
    "                    pred_logit = model(img)\n",
    "\n",
    "                    loss = criterion(pred_logit, label)\n",
    "\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    pred_label = torch.argmax(pred_logit, 1)\n",
    "                    f1=f1_score(label.to('cpu'), pred_label.to('cpu'),average='macro')\n",
    "\n",
    "                    train_loss = loss.item()\n",
    "                    train_f1=f1\n",
    "                print(\"Iter [%3d/%3d] | Train Loss %.4f | Train f1 %.4f | Valid Loss %.4f | Valid f1 %.4f\" %\n",
    "                    (plusiter, len(semi_loader), train_loss, train_f1, valid_loss, valid_f1))\n",
    "                \n",
    "\n",
    "            if (iter % 20 == 0) or (iter == len(train_loader)-1):\n",
    "                valid_loss, valid_f1 = AverageMeter(), AverageMeter()\n",
    "\n",
    "                for img, label in valid_loader:\n",
    "                    img, label = img.float().to(device), label.long().to(device)\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        pred_logit = model(img)\n",
    "\n",
    "                    loss = criterion(pred_logit, label)\n",
    "\n",
    "                    pred_label = torch.argmax(pred_logit, 1)\n",
    "                    f1=f1_score(label.to('cpu'), pred_label.to('cpu'),average='macro')\n",
    "\n",
    "                    valid_loss.update(loss.item(), len(img))\n",
    "                    valid_f1.update(f1,len(img))\n",
    "\n",
    "                valid_loss = valid_loss.avg\n",
    "                valid_f1 = valid_f1.avg\n",
    "            if (iter % 200 == 0) or (iter == len(train_loader)-1): \n",
    "                print(\"Iter [%3d/%3d] | Train Loss %.4f | Train f1 %.4f | Valid Loss %.4f | Valid f1 %.4f\" %\n",
    "                    (iter, len(train_loader), train_loss, train_f1, valid_loss, valid_f1))\n",
    "                    \n",
    "    \n",
    "        scheduler.step(valid_loss)\n",
    "\n",
    "    submission = pd.read_csv(os.path.join(test_dir, 'info.csv'))\n",
    "    image_dir = os.path.join(test_dir, 'images')\n",
    "\n",
    "    image_paths = [os.path.join(image_dir, img_id) for img_id in submission.ImageID]\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "         transforms.Resize((224,224)),\n",
    "         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]),     \n",
    "    ])\n",
    "\n",
    "    eval_dataset = TestDataset(image_paths, transform)\n",
    "\n",
    "    valid_loader = DataLoader(\n",
    "        eval_dataset,\n",
    "        shuffle=False,\n",
    "        num_workers=4\n",
    "\n",
    "    )\n",
    "\n",
    "    model.eval()\n",
    "#매 에폭 끝날 때 마다 csv 파일 하나씩 나옴.\n",
    "    all_predictions = []\n",
    "    all_possi=[]\n",
    "    all_class_possi=[]\n",
    "    for images in valid_loader:\n",
    "        with torch.no_grad():\n",
    "            images = images.to(device)\n",
    "            pred = model(images)\n",
    "            all_class_possi.append(pred[0])\n",
    "            possi=pred.max(dim=-1)[0]\n",
    "            pred = pred.argmax(dim=-1)\n",
    "            \n",
    "            all_predictions.extend(pred.cpu().numpy())\n",
    "            all_possi.append(possi.cpu().numpy())\n",
    "    submission['ans'] = all_predictions\n",
    "    submission['possi'] = all_possi\n",
    "    submission['allpossi']=all_class_possi\n",
    "    ffff='submission_forth_resnet50'\n",
    "    \n",
    "    newFilename = f'{ffff}_{kfold_num}.csv'\n",
    "    submission.to_csv(os.path.join(test_dir, newFilename), index=False)\n",
    "    print('------------------------------------------test inference is done----------------------------------------------')\n",
    "    print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2769a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a437a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
